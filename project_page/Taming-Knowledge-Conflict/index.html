<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Taming Knowledge Conflict in Language Models</title>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">
  
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/tangram.png">
  
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
  </head>
  

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Taming Knowledge Conflict in Language Models (ICML 2025 Spotlight)</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://gaotangli.github.io/" target="_blank">Gaotang Li</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://usa.visa.com/about-visa/visa-research/yuzhong-chen.html" target="_blank">Yuzhong Chen</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="http://tonghanghang.org/" target="_blank">Hanghang Tong</a><sup>1</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>University of Illinois Urbana Champaign,</span>
                    <span class="author-block"><sup>2</sup>VISA Research</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://www.arxiv.org/abs/2503.10996" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->
                
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/GaotangLi/JUICE" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>

              
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/gaotang/ParaConfilct"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>ParaConflict Benchmark</span>
                </a>
              </span>


               <!-- Twitter Link. -->
                <span class="link-block">
                  <a href="https://x.com/@GaotangLi"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-twitter"></i>
                    </span>
                    <span>Twitter</span></a>
                    </a>
                </span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Language Models (LMs) often encounter <strong>knowledge conflicts</strong> when parametric memory contradicts contextual knowledge. Previous works attribute this conflict to the interplay between "memory heads" and "context heads", attention heads assumed to promote either memory or context exclusively. In this study, we go beyond this fundamental assumption by uncovering a critical phenomenon we term the <strong>superposition of contextual information and parametric memory (CP superposition)</strong>, where highly influential attention heads simultaneously contribute to both memory and context. Building upon this insight, we propose <strong>Just Run Twice (JUICE)</strong>, a test-time attention intervention method that steers LMs toward either parametric beliefs or contextual knowledge without requiring fine-tuning. JUICE identifies a set of reliable attention heads and leverages a dual-run approach to mitigate the superposition effects. Extensive experiments across 11 datasets and 6 model architectures demonstrate that JUICE sets the new state-of-the-art performance and robust generalization, achieving significant and consistent improvement across different domains under various conflict types. Finally, we theoretically analyze knowledge conflict and CP superposition in attention heads, which further elucidates the effectiveness of JUICE in these settings.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Problem statement -->
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h2 class="title is-3">
          How can we study knowledge conflict in a clean, controlled, and rigorous setting?
        </h2>
        <p align="center">
          <img src="./static/images/Dataset_figure.jpg" class="center">
          </p>
        <div class="content has-text-justified">
          <ul>
            <li>
              <strong>Unified view of knowledge conflict</strong>
              <ul>
                <li>
                  <strong>(1) Irrelevant Context</strong> - Context is misleading while
                  parametric memory is correct; an LLM should trust its own
                  knowledge.
                </li>
                <li>
                  <strong>(2) RAG Hallucination</strong> - Context is reliable while
                  parametric memory is outdated or stubborn; an LLM should defer to
                  the context.
                </li>
              </ul>
              Depending on the application, users may want the model to
              <em>either</em> stay faithful to its parametric memory
              <em>or</em> prioritise contextual information.
            </li>

            <li>
              <strong>Fine-grained Irrelevant Context benchmark &mdash; <a href="https://huggingface.co/datasets/gaotang/ParaConfilct" target="_blank" rel="noopener noreferrer">ParaConflict</a></strong>
              <ul>
                <li>
                  We construct three evaluation tiers:  
                  (a) <strong>clean input</strong>,  
                  (b) <strong>substitution conflict</strong>, and  
                  (c) <strong>coherent conflict</strong>,  
                  representing increasing levels of contextual distraction over diverse factual domains.
                </li>
                <li>
                  This design lets us trace model internals across conflict
                  levels and test whether an intervention can steer the model
                  consistently in every case.
                </li>
              </ul>
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Results. -->
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h2 class="title is-3">Empirical Observation of the CP Superposition</h2>
        <p align="center">
        <img src="./static/images/finding_intervention.png" class="center">
        </p>
        <div class="content has-text-justified">
          Observation 1: Inconsistent Behaviors of Model Components Under Different Degrees of Knowledge Conflict.
        </div>
        <p align="center">
          <img src="./static/images/merged_table.png" class="center">
          </p>
          <div class="content has-text-justified">
            Observation 2: Counteracting Effects of Multiple Individually Effective Interventions.
          </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Results. -->
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h2 class="title is-3">Our Method: Just Run Twice (JUICE)</h2>
        <p align="center">
        <img src="./static/images/jrt.png" class="center">
        </p>
        <div class="content has-text-justified">
          JUICE operates in two stages: (1) a head identification stage, where two sets of attention heads that yield
          consistent improvements with positive or negative scaling
          are identified using a minimal number of samples, and (2) an
          dual-run inference stage, where the model runs twice: first
          saving the outputs of the identified heads, and then using
          scaled versions of these saved outputs to intervene during
          the second run. 
        </div>
      </div>
    </div>
  </div>
</section>




<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{li2025taming,
        title={Taming Knowledge Conflicts in Language Models},
        author={Li, Gaotang and Chen, Yuzhong and Tong, Hanghang},
        journal={arXiv preprint arXiv:2503.10996},
        year={2025}
      }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
